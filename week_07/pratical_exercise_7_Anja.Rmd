---
title: "practical_exercise_7 , Methods 3, 2021, autumn semester"
author: 'Anja, Astrid, Jessica, Magnus, Juli'
date: "10/11/2021"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


# Exercises and objectives

1) Estimate bias and variance based on a true underlying function  
2) Fitting training data and applying it to test sets with and without regularization  

For each question and sub-question, please indicate one of the three following answers:  
i. I understood what was required of me  
ii. I understood what was required of me, but I did not know how to fulfill the requirement  
iii. I did not understand what was required of me  

```{r}
library(reticulate)
library(Rcpp)
```


# EXERCISE 1 - Estimate bias and variance based on a true underlying function  

We can express regression as $y = f(x) + \epsilon$ with $E[\epsilon] = 0$ and $var(\epsilon) = \sigma^2$ ($E$ means expected value)  
  
For a given point: $x_0$, we can decompose the expected prediction error , $E[(y_0 - \hat{f}(x_0))^2]$ into three parts - __bias__, __variance__ and __irreducible error__ (the first two together are the __reducible error__):

The expected prediction error is, which we also call the __Mean Squared Error__:  
$E[(y_0 - \hat{f}(x_0))^2] =  bias(\hat{f}(x_0))^2 + var(\hat{f}(x_0)) + \sigma^2$
  
where __bias__ is;
  
$bias(\hat{f}(x_0)) = E[\hat{f}(x_0)] - f(x_0)$

## 1) Create a function, $f(x)$ that squares its input. This is our __true__ function  
```{python}
def f(x):
    return(x**2)
```
Rating: i

### i. generate data, $y$, based on an input range of [0, 6] with a spacing of 0.1. Call this $x$
```{python}
import numpy as np 

x = np.arange(0, 6, 0.1)
y = f(x)

```
Rating: iii, confusing when you say generate data 'y' but call 'x'

### ii. add normally distributed noise to $y$ with $\sigma=5$ (set a seed to 7 `np.random.seed(7)`) to $y$ and call it $y_{noisy}$
```{python}
np.random.seed(7)
random_vector = np.random.normal(loc=0, scale=5, size=len(x))
y_noisy = y + random_vector

# loc = mean
# scale = standard deviation
# size = number of data points 
```
Rating: i 

### iii. plot the true function and the generated points  

```{python}

import matplotlib.pyplot as plt
plt.figure()
plt.plot(x, y)
plt.plot(x, y_noisy, "ro")
plt.legend(["true function", "y_noisy"])
plt.show()
```
Rating: i 

## 2) Fit a linear regression using `LinearRegression` from `sklearn.linear_model` based on $y_{noisy}$ and $x$ (see code chunk below associated with Exercise 1.2)  
```{python}

# Below is what we tried first...and got an error about reshaping 
#from sklearn.linear_model import LinearRegression
#regressor = LinearRegression()
#regressor.fit(x, y_noisy) 

x_reshape = x.reshape(-1, 1)

from sklearn.linear_model import LinearRegression
regressor = LinearRegression() # represents the the regression model 
fit = regressor.fit(x_reshape, y_noisy) 

```
Rating: i - still a bit confused

### i. plot the fitted line (see the `.intercept_` and `.coef_` attributes of the `regressor` object) on top of the plot (from 1.1.iii)
```{python}
intercept = fit.intercept_ #getting the intercept of our model called fit 
slope  = fit.coef_ #slope of model called fit 


p = plt.figure()
plt.plot(x, y, "b-")
plt.plot(x, slope*x+intercept, "r-")
plt.plot(x, y_noisy, "k.")
plt.legend(["true function", "fitted line"])
plt.show()
```
Rating: i 

### ii. now run the code chunk below associated with Exercise 1.2.ii - what does X_quadratic amount to?
```{python}
from sklearn.preprocessing import PolynomialFeatures
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
q_fit = regressor.fit(X_quadratic, y_noisy) # quadratic fit
y_quad_hat = regressor.predict(X_quadratic)
print(X_quadratic)

```
 what does it mean - what does X_quadratic amount to...? Do not quite get this part 
Rating: ii

### iii. do a quadratic and a fifth order fit as well and plot them (on top of the plot from 1.2.i)
```{python}

fifth_order = PolynomialFeatures(degree=5)
X_fifth_order = fifth_order.fit_transform(x.reshape(-1, 1))
regressor = LinearRegression()
fit_fifth = regressor.fit(X_fifth_order, y_noisy) #fifth order fit
#predicted values
y_fit_fifth = regressor.predict(X_fifth_order)


#Plot
plt.figure() # create new figure
plt.plot(x, y, 'b-') # plot the true function
plt.plot(x, slope*x+intercept, 'r-') #linear fit
plt.plot(x, y_quad_hat, 'g-') #quadratic fit
plt.plot(x, y_fit_fifth, 'y-') #fifth order fit
plt.plot(x,y_noisy, 'k.') # generated points 
plt.xlabel('x')
plt.ylabel('y')
plt.title('Plot Exercise 2.2iii')
plt.legend(['True function', 'Linear fit', 'Quadratic fit', 'Fifth order fit', 'Generated points'])
plt.show()
```
Rating: i

## 3) Simulate 100 samples, each with sample size `len(x)` with $\sigma=5$ normally distributed noise added on top of the true function    
```{python}
samples = [] # creates an empty list 
for i in range(100):
    random_sample = y + np.random.normal(loc = 0, scale = 5, size = len(y))
    samples.append(random_sample)

```
Rating: iii?

### i. do linear, quadratic and fifth-order fits for each of the 100 samples  
```{python}
regressor = LinearRegression()

#Linear Fits 
#for i in samples
#  linear_fits = regressor.fit(x_reshape, i)

linear_fits = [regressor.fit(x_reshape, i) for i in samples]



#Quadratic Fits 
quadratic = PolynomialFeatures(degree=2)
X_quadratic = quadratic.fit_transform(x.reshape(-1, 1))
quadratic_fits = [regressor.fit(X_quadratic, i) for i in samples]

#Fifth-order fits
fifth_order = PolynomialFeatures(degree=5)
X_fifth_order = fifth_order.fit_transform(x.reshape(-1, 1)) # could we just put variable x_reshape
regressor = LinearRegression()
fifth_order_fits = quadratic_fits = [regressor.fit(X_fifth_order, i) for i in samples]
```

### ii. create a __new__ figure, `plt.figure`, and plot the linear and the quadratic fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance for $x_0$  
```{python}
plt.figure()
[plt.plot(x_reshape, i) for i in linear_fits] #linear
```
Rating: ii 
A bit unsure about this and spent enough time trying to figure it out. Ran out of time to get to the rest of the assignment. 


### iii. create a __new__ figure, `plt.figure`, and plot the quadratic and the fifth-order fits (colour them appropriately); highlight the true value for $x_0=3$. From the graphics alone, judge which fit has the highest bias and which has the highest variance for $x_0$  
### iv. estimate the __bias__ and __variance__ at $x_0$ for the linear, the quadratic and the fifth-order fits (the expected value $E[\hat{f}(x_0)] - f(x_0)$ is found by taking the mean of all the simulated, $\hat{f}(x_0)$, differences)  
### v. show how the __squared bias__ and the __variance__ is related to the complexity of the fitted models  
### vi. simulate __epsilon__: `epsilon = np.random.normal(scale=5, size=100)`. Based on your simulated values of __bias, variance and epsilon__, what is the __Mean Squared Error__ for each of the three fits? Which fit is better according to this measure? 



# EXERCISE 2: Fitting training data and applying it to test sets with and without regularization

All references to pages are made to this book:
Raschka, S., 2015. Python Machine Learning. Packt Publishing Ltd.  

## 1) Import the housing dataset using the upper chunk of code from p. 280 
```{python}
import pandas as pd
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\s+')
df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS',
  'NOX', 'RM', 'AGE', 'DIS', 'RAD',
  'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']
df.head()
```
### i. and define the correlation matrix `cm` as done on p. 284  
```{python}
#First we import sns and define cols, which is used on p. 280+281 and is needed for this code chunk
import seaborn as sns
cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV']
import numpy as np
cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.5)
hm = sns.heatmap(cm,
   cbar=True,
   annot=True,
   square=True,
   fmt='.2f',
   annot_kws={'size': 15},
   yticklabels=cols,
   xticklabels=cols)
plt.show()
```
### ii. based on this matrix, do you expect collinearity can be an issue if we run multiple linear regression  by fitting MEDV on LSTAT, INDUS, NOX and RM?  
## 2) Fit MEDV on  LSTAT, INDUS, NOX and RM (standardize all five variables by using `StandardScaler.fit_transform`, (`from sklearn.preprocessing import StandardScaler`) by doing multiple linear regression using `LinearRegressionGD` as defined on pp. 285-286
```{python}
#First we standardise
from sklearn.preprocessing import StandardScaler
StandardScaler.fit_transform(cols)
#Now we do multiple linear regression
class LinearRegressionGD(object):
       def __init__(self, eta=0.001, n_iter=20):
           self.eta = eta
           self.n_iter = n_iter
       def fit(self, X, y):
           self.w_ = np.zeros(1 + X.shape[1])
           self.cost_ = []
           for i in range(self.n_iter):
               output = self.net_input(X)
               errors = (y - output)
               self.w_[1:] += self.eta * X.T.dot(errors)
               self.w_[0] += self.eta * errors.sum()
               cost = (errors**2).sum() / 2.0
               self.cost_.append(cost)
return self
       def net_input(self, X):
           return np.dot(X, self.w_[1:]) + self.w_[0]
       def predict(self, X):
           return self.net_input(X)
X = df[['RM']].values
y = df['MEDV'].values
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y)
lr = LinearRegressionGD()
lr.fit(X_std, y_std)           
```
### i. how much does the solution improve in terms of the cost function if you go through 40 iterations instead of the default of 20 iterations?  
```{python}
#Changing the function so there are 40 iterations instead of 20
class LinearRegressionGD(object):
       def __init__(self, eta=0.001, n_iter=20):
           self.eta = eta
           self.n_iter = n_iter
       def fit(self, X, y):
           self.w_ = np.zeros(1 + X.shape[1])
           self.cost_ = []
           for i in range(self.n_iter):
               output = self.net_input(X)
               errors = (y - output)
               self.w_[1:] += self.eta * X.T.dot(errors)
               self.w_[0] += self.eta * errors.sum()
               cost = (errors**2).sum() / 2.0
               self.cost_.append(cost)
return self
       def net_input(self, X):
           return np.dot(X, self.w_[1:]) + self.w_[0]
       def predict(self, X):
           return self.net_input(X)
X = df[['RM']].values
y = df['MEDV'].values
from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y)
lr = LinearRegressionGD()
lr.fit(X_std, y_std)           
```
### ii. how does the residual sum of squares based on the analytic solution (Ordinary Least Squares) compare to the cost after 40 iterations?
### iii. Bonus question: how many iterations do you need before the Ordinary Least Squares and the Gradient Descent solutions result in numerically identical residual sums of squares?  
## 3) Build your own cross-validator function. This function should randomly split the data into $k$ equally sized folds (see figure p. 176) (see the code chunk associated with exercise 2.3). It should also return the Mean Squared Error for each of the folds
```{python, eval=FALSE}
# Exercise 2.3
def cross_validate(estimator, X, y, k): # estimator is the object created by initialising LinearRegressionGD
    mses = list() # we want to return k mean squared errors
    fold_size = y.shape[0] // k # we do integer division to get a whole number of samples
    for fold in range(k): # loop through each of the folds
        
        X_train = ?
        y_train = ?
        X_test = ?
        y_test = ?
        
        # fit training data
        # predict on test data
        # calculate MSE
        
    return mses
```
### i. Cross-validate the fits of your model from Exercise 2.2. Run 11 folds and run 500 iterations for each fit  
### ii. What is the mean of the mean squared errors over all 11 folds?  
## 4) Now, we will do a Ridge Regression. Use `Ridge` (see code chunk associated with Exercise 2.4) to find the optimal `alpha` parameter ($\lambda$)

```{python, eval=FALSE}
# Exercise 2.4
from sklearn.linear_model import Ridge, Lasso
RR = Ridge(alpha=?)
LassoR = Lasso(alpha)
```
### i. Find the _MSE_ (the mean of the _MSE's_ associated with each fold) associated with a reasonable range of `alpha` values (you need to find the lambda that results in the minimum _MSE_)  
### ii. Plot the _MSE_ as a function of `alpha` ($\lambda$). Make sure to include an _MSE_ for `alpha=0` as well  
### iii. Find the _MSE_ for the optimal `alpha`, compare its _MSE_ to that of the OLS regression
### iv. Do the same steps for Lasso Regression `Lasso`  (2.4.i.-2.4.iii.)
### v. Describe the differences between these three models, (the optimal Lasso, the optimal Ridge and the OLS)



